Index: 算法/布隆过滤器.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\n\n### 哈希表\n\n> 对于数据去重存在很多算法，首先想到的就是哈希表。通过散列函数将可能出现的数据通过哈希值映射到表中的一个位置，通过哈希值可以加快查找速度。\n\n一个典型的应用就是`HashMap`\n\n存在以下特点\n\n- 如果哈希值不等，那么对应的数据也是不等的\n- 如果哈希值相等，那么对应的元素不一定不相等（存在碰撞的可能性）\n\n#### hash表去重\n\n> 利用`hashMap`快速查找的特性，我们可以利用其进行去重操作\n\n```java\n/**\n * 对数组去重\n *\n * @param source\n */\npublic static void withoutRepeat(int[] source) {\n    final int[] temp = new int[source.length];\n    final Map<Integer, Boolean> map = new HashMap<>();\n    int index = 0;\n    for (int element : source) {\n        if (!map.containsKey(element)) {\n            map.put(element, true);\n            temp[index++] = element;\n        }\n    }\n    System.arraycopy(temp, 0, source, 0, temp.length);\n}\n```\n\n\n\n#### 缺点\n\n> 对于大数据去重，哈希表就不再适用。\n\n- 哈希表不仅仅只能做标记一个数据是否出现过，可以存储一个数据的附属状态，比如计算一个数据出现的次数，那么使用哈希表来去重有点大材小用了\n- 哈希表的空间利用率只用50%，和其加载因子有关，之所以叫散列表是应为数据不连续，且哈希表也不是存满数据的\n\n\n\n<hr>\n\n\n\n### bitset\n\n> 称为`位集`，是一种空间利用率很高的数据结构。而且所做的事情很存粹只是用于记录某个值是否出现过，通过1、0来判断。\n\n#### 原理\n\nBitSet使用一个long[]数组来存储元素状态，内部频繁使用位运算来确定元素对应数组下标，再通过或运算来修改该元素对应状态为1，一个long可以存储64个元素状态。极致的压缩了空间。但也存在缺点如果说元素值分布不均匀值与值之间存在较大的跨度，就会导致空间的利用率没有想象的那么大。\n\n![image-20220709011717422](布隆过滤器.assets/image-20220709011717422.png)\n\n#### 去重\n\n```java\npublic static int[] withoutRepeat(int[] source) {\n    final BitSet bitSet = new BitSet();\n    for (Integer element : source) {\n        bitSet.set(element);\n    }\n    final int[] target = bitSet.stream().toArray();\n    System.arraycopy(target, 0, source, 0, target.length);\n    return target;\n}\n```\n\n\n\n#### 优缺点\n\n优点：\n\n> bitset的空间复杂度不随原集合内个数的增加而增加\n\n缺点\n\n> bitset的空间复杂度随原集合内最大元素大小增加而增加\n\n\n\n### 布隆过滤器\n\n> 对于一个对象判断是否重复使用`BitSet`的话，一般会使用`BitSet`存储对象对应的哈希值，但是就会有一个问题：如果说这个对象对应的哈希算法很糟糕，出现碰撞的概率很高的话，那么BitSet出现误判的几率就很高。\n>\n> 应运而生的布隆过滤器(Bloom Filter)就出现了，其核心思想就是使用多个哈希函数来降低误判概率\n\n#### 实现\n\n- 哈希算法使用加权哈希\n- 默认八次哈希，减少误判率\n- bitset位数固定2^24，避免数据过大，降低空间利用率\n\n```java\npublic class BloomFilter {\n\n    /**\n     * bitSet分配2^24位\n     */\n    private static final int DEFAULT_SIZE = 1 << 25;\n\n    /**\n     * 哈希种子，循环加权次数\n     */\n    private static final int[] SEEDS = new int[]{3, 5, 7, 11, 13, 31, 37, 61};\n    /**\n     * 位集，给定初始化范围，避免频繁扩容\n     */\n    private final BitSet bits = new BitSet(DEFAULT_SIZE);\n\n    /**\n     * 多次哈希的哈希函数\n     */\n    private final SimpleHash[] func = new SimpleHash[SEEDS.length];\n\n    /**\n     * 布隆过滤器，初始化哈希函数\n     */\n    public BloomFilter() {\n        for (int i = 0; i < SEEDS.length; i++) {\n            func[i] = new SimpleHash(DEFAULT_SIZE, SEEDS[i]);\n        }\n    }\n\n    /**\n     * 将字符串对应多次哈希结果记录到bitSet中\n     */\n    public void add(String value) {\n        for (SimpleHash f : func) {\n            bits.set(f.hash(value), true);\n        }\n    }\n\n    /**\n     * 判断是否重复\n     */\n    public boolean contains(String value) {\n        if (value == null) {\n            return false;\n        }\n        boolean ret = true;\n        for (SimpleHash f : func) {\n            ret = ret && bits.get(f.hash(value));\n            //存在一次为0直接返回\n            if (!ret) {\n                break;\n            }\n        }\n        return ret;\n    }\n\n    /**\n     * 哈希函数类\n     */\n    @Data\n    @AllArgsConstructor\n    public static class SimpleHash {\n        private final int cap;\n        private final int seed;\n\n        //采用简单的加权和hash\n        public int hash(String value) {\n            int result = 0;\n            int len = value.length();\n            for (int i = 0; i < len; i++) {\n                result = seed * result + value.charAt(i);\n            }\n            //防止哈希过大\n            return (cap - 1) & result;\n        }\n    }\n}\n```\n\n#### 使用\n\n> 对1000万个url进行去重\n\n\n\n##### 生成10万个url\n\n> 99的倍数则写两次，模拟重复数据\n\n```java\n/**\n * 生成1000万个url\n */\n@Test\npublic void test() {\n    String filePath = \"/Users/rolyfish/Desktop/MyFoot/testfile\";\n    String fileName = \"urls.txt\";\n    int urlNum = 10000000;\n    String baseUrl = \"https://www.baidu/baidu/article/details/\";\n    try (FileWriter fileWriter = new FileWriter(new File(filePath, fileName), true)) {\n        //生成100000个url\n        for (int i = 0; i < urlNum; i++) {\n            //写两次\n            if (i % 99 == 0) {\n                fileWriter.write(baseUrl + (urlNum + i));\n                fileWriter.write(System.lineSeparator());\n                fileWriter.flush();\n            }\n            fileWriter.write(baseUrl + (urlNum + i));\n            fileWriter.write(System.lineSeparator());\n            fileWriter.flush();\n        }\n        fileWriter.flush();\n        fileWriter.close();\n    } catch (IOException e) {\n        System.out.println();\n    }\n}\n```\n\n这里是存在重复数据的,而且文件还是挺大的。\n\n![image-20220709043157614](布隆过滤器.assets/image-20220709043157614.png)\n\n![image-20220709043225418](布隆过滤器.assets/image-20220709043225418.png)\n\n##### 测试去重\n\n> 读取url文件，一行一行读，判断是否重复，不重复写入新的文件中。\n\n```java\n/**\n * 使用此布隆过滤器对10万个url去重\n */\npublic static void main(String[] args) {\n    final long start = Calendar.getInstance().getTime().getTime();\n    final BloomFilter bloomFilter = new BloomFilter();\n    String filePath = \"/Users/rolyfish/Desktop/MyFoot/testfile\";\n    String fileName = \"urls.txt\";\n    String fileNameNew = \"urlsNew.txt\";\n    final File fileUrl = new File(filePath, fileName);\n    final File fileUtlNew = new File(filePath, fileNameNew);\n    if (!fileUrl.exists()) {\n        System.out.println(\"文件不存在\");\n    }\n    String buffer = \"\";\n    try (\n            final FileReader fileReader = new FileReader(fileUrl);\n            final BufferedReader bufferedReader = new BufferedReader(fileReader);\n            final FileWriter fileWriter = new FileWriter(fileUtlNew)) {\n        while ((buffer = bufferedReader.readLine()) != null) {\n            if (!bloomFilter.contains(buffer)){\n                fileWriter.write(buffer);\n                fileWriter.write(System.lineSeparator());\n                fileWriter.flush();\n            }\n            //添加到布隆过滤器\n            bloomFilter.add(buffer);\n        }\n        fileWriter.close();\n        fileReader.close();\n    } catch (IOException e) {\n        System.out.println(\"io异常\");\n    }\n  \tfinal long end = Calendar.getInstance().getTime().getTime();\n  \tSystem.out.println(end - start);\n}\n```\n\n实现了去重，且速度是可观的。\n\n![image-20220709043424006](布隆过滤器.assets/image-20220709043424006.png)\n\n![image-20220709043435754](布隆过滤器.assets/image-20220709043435754.png)\n\n![image-20220709043713291](布隆过滤器.assets/image-20220709043713291.png)\n\n\n\n##### Google提供的布隆过滤器\n\n> Google为我们提供的布隆过滤器，位于`com.google.common.hash`包下。原理相似。\n\n依赖\n\n```xml\n<dependency>\n    <groupId>com.google.guava</groupId>\n    <artifactId>guava</artifactId>\n    <version>31.1-jre</version>\n</dependency>\n```\n\n\n\n使用：\n\n```java\npublic static void main(String[] args) {\n    /**\n     * 参数说明：\n     *  - 参数漏斗\n     *  - 预期插入多少次\n     *  - 预期误判率\n     */\n    BloomFilter<String> filter = BloomFilter.create(Funnels.stringFunnel(Charset.defaultCharset()), 100, 0.01);\n\n    /**\n     * 将元素放入布隆过滤器\n     * - 位集发生改变则一定是首次添加返回true\n     * - 位集未发生改变，可能不是首次添加（存在误判），返回false\n     */\n    filter.put(\"123\");\n\n    /**\n     * 判断元素是否存在于位集\n     * - false 一定不存在\n     * - true  可能存在（存在误判）\n     */\n    filter.test(\"123\");\n}\n```
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/算法/布隆过滤器.md b/算法/布隆过滤器.md
--- a/算法/布隆过滤器.md	
+++ b/算法/布隆过滤器.md	
@@ -1,5 +1,3 @@
-
-
 ### 哈希表
 
 > 对于数据去重存在很多算法，首先想到的就是哈希表。通过散列函数将可能出现的数据通过哈希值映射到表中的一个位置，通过哈希值可以加快查找速度。
